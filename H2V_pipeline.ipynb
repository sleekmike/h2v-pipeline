{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4fmDrJUuXzD"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade \"scenedetect[opencv]\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe==0.10.14 librosa"
      ],
      "metadata": {
        "id": "_hwT0ARFuY9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make sure to upload the \"input_video.mp4\" file, before running the cell below:"
      ],
      "metadata": {
        "id": "WRDAN_i2ukDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import scenedetect\n",
        "from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "import mediapipe as mp\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "# Import the audio_classifier module correctly\n",
        "from mediapipe.tasks.python.audio import audio_classifier\n",
        "\n",
        "class H2VProcessor:\n",
        "    def __init__(self):\n",
        "        # Initialize MediaPipe solutions\n",
        "        self.mp_face_detection = mp.solutions.face_detection\n",
        "        self.mp_face_detection_model = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
        "        self.mp_pose = mp.solutions.pose\n",
        "        self.mp_pose_model = self.mp_pose.Pose(min_detection_confidence=0.5)\n",
        "\n",
        "        # Tracking parameters\n",
        "        self.prev_focus_point = None\n",
        "        self.focus_points_data = []\n",
        "\n",
        "    def detect_scenes(self, video_path):\n",
        "        \"\"\"Split video into scenes using PySceneDetect\"\"\"\n",
        "        video_manager = VideoManager([video_path])\n",
        "        scene_manager = SceneManager()\n",
        "        scene_manager.add_detector(ContentDetector(threshold=30))\n",
        "\n",
        "        video_manager.start()\n",
        "        scene_manager.detect_scenes(frame_source=video_manager)\n",
        "\n",
        "        scene_list = scene_manager.get_scene_list()\n",
        "        return scene_list\n",
        "\n",
        "    def detect_faces(self, frame):\n",
        "        \"\"\"Detect faces in the frame\"\"\"\n",
        "        results = self.mp_face_detection_model.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        faces = []\n",
        "        if results.detections:\n",
        "            for detection in results.detections:\n",
        "                bboxC = detection.location_data.relative_bounding_box\n",
        "                ih, iw, _ = frame.shape\n",
        "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
        "                             int(bboxC.width * iw), int(bboxC.height * ih)\n",
        "                faces.append((x, y, w, h))\n",
        "        return faces\n",
        "\n",
        "    def detect_people(self, frame):\n",
        "        \"\"\"Detect people using pose estimation\"\"\"\n",
        "        results = self.mp_pose_model.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        people = []\n",
        "        if results.pose_landmarks:\n",
        "            h, w, _ = frame.shape\n",
        "            # Use nose landmark as person center\n",
        "            if results.pose_landmarks.landmark[0].visibility > 0.5:  # Check if nose is visible\n",
        "                nose_x = int(results.pose_landmarks.landmark[0].x * w)\n",
        "                nose_y = int(results.pose_landmarks.landmark[0].y * h)\n",
        "                people.append((nose_x, nose_y))\n",
        "        return people\n",
        "\n",
        "    def detect_motion(self, prev_frame, curr_frame):\n",
        "        \"\"\"Detect motion between frames using optical flow\"\"\"\n",
        "        if prev_frame is None:\n",
        "            return None\n",
        "\n",
        "        # Convert frames to grayscale\n",
        "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Calculate optical flow\n",
        "        flow = cv2.calcOpticalFlowFarneback(prev_gray, curr_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "        # Get magnitude and angle of flow\n",
        "        magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "        # Find area with most motion\n",
        "        if np.max(magnitude) > 0.5:  # Only if significant motion exists\n",
        "            y, x = np.unravel_index(magnitude.argmax(), magnitude.shape)\n",
        "            return (x, y)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_focus_point(self, frame, prev_frame=None, is_speaking=False):\n",
        "        \"\"\"Determine the main focus point in the frame\"\"\"\n",
        "        h, w, _ = frame.shape\n",
        "\n",
        "        # Priority 1: Speaking person\n",
        "        if is_speaking:\n",
        "            faces = self.detect_faces(frame)\n",
        "            if faces:\n",
        "                # Return center of the largest face\n",
        "                largest_face = max(faces, key=lambda face: face[2] * face[3])\n",
        "                x, y, fw, fh = largest_face\n",
        "                return (x + fw // 2, y + fh // 2)\n",
        "\n",
        "        # Priority 2: People in frame\n",
        "        people = self.detect_people(frame)\n",
        "        if people:\n",
        "            # Return position of first detected person\n",
        "            return people[0]\n",
        "\n",
        "        # Priority 3: Motion detection\n",
        "        motion_point = self.detect_motion(prev_frame, frame)\n",
        "        if motion_point:\n",
        "            return motion_point\n",
        "\n",
        "        # Fallback: use previous focus point or center of frame\n",
        "        if self.prev_focus_point:\n",
        "            return self.prev_focus_point\n",
        "\n",
        "        return (w // 2, h // 2)\n",
        "\n",
        "    def smooth_focus_point(self, new_point, alpha=0.3):\n",
        "        \"\"\"Apply smoothing to prevent jittery camera movement\"\"\"\n",
        "        if self.prev_focus_point is None:\n",
        "            self.prev_focus_point = new_point\n",
        "            return new_point\n",
        "\n",
        "        # Exponential smoothing\n",
        "        smoothed_x = int(alpha * new_point[0] + (1 - alpha) * self.prev_focus_point[0])\n",
        "        smoothed_y = int(alpha * new_point[1] + (1 - alpha) * self.prev_focus_point[1])\n",
        "\n",
        "        self.prev_focus_point = (smoothed_x, smoothed_y)\n",
        "        return self.prev_focus_point\n",
        "\n",
        "    def crop_to_vertical(self, frame, focus_point):\n",
        "        \"\"\"Crop the frame to vertical 9:16 aspect ratio centered on focus point\"\"\"\n",
        "        h, w = frame.shape[:2]\n",
        "\n",
        "        # Calculate crop dimensions for 9:16 aspect ratio\n",
        "        target_width = int(h * 9 / 16)\n",
        "\n",
        "        # Center the crop on the focus point\n",
        "        x_center, y_center = focus_point\n",
        "\n",
        "        # Calculate crop boundaries\n",
        "        left = max(0, x_center - target_width // 2)\n",
        "        right = min(w, left + target_width)\n",
        "\n",
        "        # Adjust if we hit the edge of the frame\n",
        "        if right == w:\n",
        "            left = max(0, w - target_width)\n",
        "        if left == 0:\n",
        "            right = min(w, target_width)\n",
        "\n",
        "        # Perform the crop\n",
        "        cropped = frame[:, left:right]\n",
        "\n",
        "        # Return both the cropped frame and focus point data\n",
        "        return cropped, {\n",
        "            \"time\": None,  # Will be set later\n",
        "            \"original_frame\": {\"width\": w, \"height\": h},\n",
        "            \"focus_point\": {\"x\": x_center, \"y\": y_center},\n",
        "            \"crop\": {\"left\": left, \"right\": right, \"top\": 0, \"bottom\": h}\n",
        "        }\n",
        "\n",
        "    def process_video(self, input_path, output_path, fps=None, temp_dir=\"temp_frames\"):\n",
        "        \"\"\"Process the whole video and generate vertical version with tracking data\"\"\"\n",
        "        # Create temp directory for frames if it doesn't exist\n",
        "        os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        if not cap.isOpened():\n",
        "            raise ValueError(f\"Could not open video file: {input_path}\")\n",
        "\n",
        "        # Get video properties\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Use original fps if not specified\n",
        "        if fps is None:\n",
        "            fps = original_fps\n",
        "\n",
        "        # Detect scenes\n",
        "        scenes = self.detect_scenes(input_path)\n",
        "        if not scenes:\n",
        "            # If no scenes detected, treat the whole video as one scene\n",
        "            scenes = [(0, total_frames)]\n",
        "\n",
        "        # Output dimensions for 9:16 aspect ratio based on original height\n",
        "        out_width = int(height * 9 / 16)\n",
        "\n",
        "        # Process each scene\n",
        "        prev_frame = None\n",
        "        frame_count = 0\n",
        "\n",
        "        # Reset tracking for new video\n",
        "        self.prev_focus_point = None\n",
        "        self.focus_points_data = []\n",
        "\n",
        "        # Create temporary video path for frames only\n",
        "        temp_video_path = os.path.splitext(output_path)[0] + \"_temp.mp4\"\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(temp_video_path, fourcc, fps, (out_width, height))\n",
        "\n",
        "        with tqdm(total=total_frames, desc=\"Processing frames\") as progress_bar:\n",
        "            while True:\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "\n",
        "                # Calculate current time in seconds\n",
        "                current_time = frame_count / original_fps\n",
        "\n",
        "                # Get focus point\n",
        "                focus_point = self.get_focus_point(frame, prev_frame)\n",
        "\n",
        "                # Apply smoothing\n",
        "                smooth_point = self.smooth_focus_point(focus_point)\n",
        "\n",
        "                # Crop frame\n",
        "                cropped_frame, focus_data = self.crop_to_vertical(frame, smooth_point)\n",
        "\n",
        "                # Add timestamp to focus data\n",
        "                focus_data[\"time\"] = current_time\n",
        "                self.focus_points_data.append(focus_data)\n",
        "\n",
        "                # Write frame to output video\n",
        "                out.write(cropped_frame)\n",
        "\n",
        "                # Update for next iteration\n",
        "                prev_frame = frame.copy()\n",
        "                frame_count += 1\n",
        "                progress_bar.update(1)\n",
        "\n",
        "        # Clean up video capture and writer\n",
        "        cap.release()\n",
        "        out.release()\n",
        "\n",
        "        # Combine the processed video with the original audio using FFmpeg\n",
        "        self.combine_video_with_audio(input_path, temp_video_path, output_path)\n",
        "\n",
        "        # Remove temporary video file\n",
        "        if os.path.exists(temp_video_path):\n",
        "            os.remove(temp_video_path)\n",
        "\n",
        "        # Export focus points data as JSON\n",
        "        tracking_data_path = os.path.splitext(output_path)[0] + \"_tracking.json\"\n",
        "        with open(tracking_data_path, 'w') as f:\n",
        "            json.dump({\"focus_points\": self.focus_points_data}, f, indent=2)\n",
        "\n",
        "        return output_path, tracking_data_path\n",
        "\n",
        "    def combine_video_with_audio(self, original_video, processed_video, output_path):\n",
        "        \"\"\"Combine the processed video with the original audio using FFmpeg\"\"\"\n",
        "        try:\n",
        "            print(\"Adding audio track to the processed video...\")\n",
        "            # Command to extract audio from original video and combine with new video\n",
        "            ffmpeg_cmd = [\n",
        "                'ffmpeg',\n",
        "                '-i', processed_video,  # Input processed video (no audio)\n",
        "                '-i', original_video,   # Input original video (for audio)\n",
        "                '-c:v', 'copy',         # Copy video stream without re-encoding\n",
        "                '-c:a', 'aac',          # Use AAC codec for audio\n",
        "                '-map', '0:v:0',        # Use video from first input\n",
        "                '-map', '1:a:0',        # Use audio from second input\n",
        "                '-shortest',            # Finish encoding when the shortest input stream ends\n",
        "                '-y',                   # Overwrite output file if it exists\n",
        "                output_path\n",
        "            ]\n",
        "\n",
        "            # Run the FFmpeg command\n",
        "            subprocess.run(ffmpeg_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            print(f\"Successfully added audio to: {output_path}\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error adding audio: {e}\")\n",
        "            print(f\"FFmpeg stderr: {e.stderr.decode() if e.stderr else 'No error output'}\")\n",
        "            # If FFmpeg fails, just use the video without audio\n",
        "            import shutil\n",
        "            shutil.copy(processed_video, output_path)\n",
        "            print(f\"Using video without audio: {output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error adding audio: {e}\")\n",
        "            # If another error occurs, just use the video without audio\n",
        "            import shutil\n",
        "            shutil.copy(processed_video, output_path)\n",
        "            print(f\"Using video without audio: {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    processor = H2VProcessor()\n",
        "\n",
        "    input_video = \"input_video.mp4\"  # Replace with your video path\n",
        "    output_video = \"output_vertical2_video.mp4\"\n",
        "\n",
        "    video_path, tracking_data = processor.process_video(input_video, output_video)\n",
        "    print(f\"Processing complete. Vertical video saved to: {video_path}\")\n",
        "    print(f\"Tracking data saved to: {tracking_data}\")"
      ],
      "metadata": {
        "id": "IiA_SS4rueaH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}