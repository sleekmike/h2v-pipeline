{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4fmDrJUuXzD",
        "outputId": "7cd7b0e2-3b04-497b-ae14-1420b1390e49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scenedetect[opencv] in /usr/local/lib/python3.11/dist-packages (0.6.6)\n",
            "Requirement already satisfied: Click in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (8.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (2.0.2)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from scenedetect[opencv]) (4.11.0.86)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade \"scenedetect[opencv]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hwT0ARFuY9E",
        "outputId": "41bfa385-3fe6-4dab-b5d1-7abd749879ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mediapipe==0.10.14 in /usr/local/lib/python3.11/dist-packages (0.10.14)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (2.0.2)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.14) (0.5.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.14) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.14) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.14) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.14) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.14) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.14) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.14) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.14) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.14) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.14) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.14) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.14) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe==0.10.14 librosa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSjP-kSp3aJZ"
      },
      "source": [
        "### **Download Test Video from Google Drive Link**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5V69ZMG1zer",
        "outputId": "aa38dea5-30bc-4289-d48f-7ddd8519c58b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Bw3MkBhJ_TusVyJ7Po23aiTOCMIc0bYx\n",
            "From (redirected): https://drive.google.com/uc?id=1Bw3MkBhJ_TusVyJ7Po23aiTOCMIc0bYx&confirm=t&uuid=28ab91ba-f180-4782-9cfe-333960ea1ff7\n",
            "To: /content/input_video.mp4\n",
            "100% 187M/187M [00:03<00:00, 47.1MB/s]\n",
            "Video downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Download the video from Google Drive\n",
        "!gdown --id 1Bw3MkBhJ_TusVyJ7Po23aiTOCMIc0bYx -O /content/input_video.mp4\n",
        "\n",
        "# Verify if the file exists\n",
        "if os.path.exists(\"/content/input_video.mp4\"):\n",
        "  print(\"Video downloaded successfully!\")\n",
        "else:\n",
        "  print(\"Video download failed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-yL_1oeKryH"
      },
      "source": [
        "### **UTILITY FILES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBJsA9hHW0_p"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class VirtualCamera:\n",
        "    \"\"\"\n",
        "    Simulates physical camera movement with momentum and damping for smooth tracking.\n",
        "    This creates natural, cinematographic movements by simulating camera physics.\n",
        "    \"\"\"\n",
        "    def __init__(self, initial_position=(0, 0), damping=0.85, spring=0.15, mass=1.0):\n",
        "        # Camera position and movement properties\n",
        "        self.position = initial_position  # Current center position\n",
        "        self.velocity = (0, 0)  # Current velocity vector\n",
        "        self.target = initial_position  # Target position\n",
        "\n",
        "        # Physics parameters\n",
        "        self.mass = mass  # Virtual camera \"weight\"\n",
        "        self.damping = damping  # Damping factor (higher = smoother but slower)\n",
        "        self.spring = spring  # Spring factor (higher = faster snap to target)\n",
        "\n",
        "        # Additional stability controls\n",
        "        self.min_movement_threshold = 0.5  # Ignore tiny movements below this threshold\n",
        "        self.position_history = deque(maxlen=30)\n",
        "        self.target_history = deque(maxlen=30)\n",
        "\n",
        "        # Shot transition properties\n",
        "        self.in_transition = False\n",
        "        self.transition_progress = 0\n",
        "        self.transition_start_pos = None\n",
        "        self.transition_target = None\n",
        "        self.transition_duration = 30  # frames\n",
        "\n",
        "    def update(self, target_pos, force_immediate=False):\n",
        "        \"\"\"\n",
        "        Update camera position with physics simulation.\n",
        "        Returns the new camera position as (x, y) integers.\n",
        "\n",
        "        Args:\n",
        "            target_pos: The target position to move toward\n",
        "            force_immediate: If True, jump immediately to target (for scene cuts)\n",
        "        \"\"\"\n",
        "        self.target = target_pos\n",
        "        self.target_history.append(target_pos)\n",
        "\n",
        "        # For scene cuts or initialization, jump immediately\n",
        "        if force_immediate:\n",
        "            self.position = target_pos\n",
        "            self.velocity = (0, 0)\n",
        "            self.position_history.clear()\n",
        "            self.position_history.append(self.position)\n",
        "            return (int(self.position[0]), int(self.position[1]))\n",
        "\n",
        "        # Handle shot transitions with easing\n",
        "        if self.in_transition:\n",
        "            self.transition_progress += 1\n",
        "            if self.transition_progress >= self.transition_duration:\n",
        "                self.in_transition = False\n",
        "                self.position = self.transition_target\n",
        "            else:\n",
        "                # Cubic easing (smooth acceleration and deceleration)\n",
        "                t = self.transition_progress / self.transition_duration\n",
        "                t = t * t * (3 - 2 * t)  # Cubic easing formula\n",
        "\n",
        "                # Interpolate between start and target positions\n",
        "                self.position = (\n",
        "                    self.transition_start_pos[0] + (self.transition_target[0] - self.transition_start_pos[0]) * t,\n",
        "                    self.transition_start_pos[1] + (self.transition_target[1] - self.transition_start_pos[1]) * t\n",
        "                )\n",
        "                self.position_history.append(self.position)\n",
        "                return (int(self.position[0]), int(self.position[1]))\n",
        "\n",
        "        # Check if target has been stable for a while and we're approaching it\n",
        "        if len(self.target_history) > 10:\n",
        "            recent_targets = list(self.target_history)[-10:]\n",
        "            target_x_std = np.std([t[0] for t in recent_targets])\n",
        "            target_y_std = np.std([t[1] for t in recent_targets])\n",
        "\n",
        "            # If target is very stable and we're close, lock onto it more firmly\n",
        "            distance_to_target = np.sqrt((self.position[0] - target_pos[0])**2 +\n",
        "                                        (self.position[1] - target_pos[1])**2)\n",
        "\n",
        "            if target_x_std < 5 and target_y_std < 5 and distance_to_target < 20:\n",
        "                # Increase spring force for faster convergence on stable targets\n",
        "                spring = self.spring * 2\n",
        "            else:\n",
        "                spring = self.spring\n",
        "        else:\n",
        "            spring = self.spring\n",
        "\n",
        "        # Calculate spring force toward target (with deadzone for tiny movements)\n",
        "        dx = self.target[0] - self.position[0]\n",
        "        dy = self.target[1] - self.position[1]\n",
        "\n",
        "        # Apply deadzone to reduce jitter\n",
        "        if abs(dx) < self.min_movement_threshold:\n",
        "            dx = 0\n",
        "        if abs(dy) < self.min_movement_threshold:\n",
        "            dy = 0\n",
        "\n",
        "        force_x = dx * spring\n",
        "        force_y = dy * spring\n",
        "\n",
        "        # Apply force to velocity (F = ma)\n",
        "        accel_x = force_x / self.mass\n",
        "        accel_y = force_y / self.mass\n",
        "\n",
        "        # Update velocity with acceleration and damping\n",
        "        self.velocity = (\n",
        "            self.velocity[0] * self.damping + accel_x,\n",
        "            self.velocity[1] * self.damping + accel_y\n",
        "        )\n",
        "\n",
        "        # Update position\n",
        "        new_x = self.position[0] + self.velocity[0]\n",
        "        new_y = self.position[1] + self.velocity[1]\n",
        "        self.position = (new_x, new_y)\n",
        "\n",
        "        # Add to position history\n",
        "        self.position_history.append(self.position)\n",
        "\n",
        "        return (int(self.position[0]), int(self.position[1]))\n",
        "\n",
        "    def start_transition(self, target_pos, duration=30):\n",
        "        \"\"\"Start a smooth transition to a new position (e.g., for scene changes)\"\"\"\n",
        "        self.in_transition = True\n",
        "        self.transition_progress = 0\n",
        "        self.transition_start_pos = self.position\n",
        "        self.transition_target = target_pos\n",
        "        self.transition_duration = duration\n",
        "\n",
        "    def get_long_term_stability_factor(self):\n",
        "        \"\"\"\n",
        "        Calculate how stable the camera has been recently.\n",
        "        Returns a value between 0 (unstable) and 1 (very stable).\n",
        "        \"\"\"\n",
        "        if len(self.position_history) < 10:\n",
        "            return 0.5  # Default mid-range value when history is short\n",
        "\n",
        "        recent_positions = list(self.position_history)[-10:]\n",
        "        x_std = np.std([p[0] for p in recent_positions])\n",
        "        y_std = np.std([p[1] for p in recent_positions])\n",
        "\n",
        "        # Normalize the stability factor (lower std = higher stability)\n",
        "        stability = 1.0 - min(1.0, (x_std + y_std) / 50.0)\n",
        "        return stability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuqbUYEjsskQ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class SceneContentAnalyzer:\n",
        "    \"\"\"\n",
        "    Analyzes scene content to determine the best framing strategy.\n",
        "    Handles scenes with and without people, identifying key visual elements.\n",
        "    \"\"\"\n",
        "    def __init__(self, face_detector, pose_detector):\n",
        "        self.face_detector = face_detector\n",
        "        self.pose_detector = pose_detector\n",
        "\n",
        "        # For saliency detection (non-human scenes)\n",
        "        self.saliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
        "\n",
        "        # Content type thresholds\n",
        "        self.people_presence_threshold = 0.25  # Min ratio of frames that must contain people\n",
        "        self.min_scene_frames = 5  # Minimum frames to analyze for a scene\n",
        "\n",
        "    def analyze_scene_frames(self, frames):\n",
        "        \"\"\"\n",
        "        Analyze a collection of frames from a scene to determine content type and best framing.\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis results including content_type and framing strategy\n",
        "        \"\"\"\n",
        "        if len(frames) < self.min_scene_frames:\n",
        "            # Default to center framing for very short scenes\n",
        "            return {\n",
        "                \"content_type\": \"unknown\",\n",
        "                \"framing_strategy\": \"center\",\n",
        "                \"people_ratio\": 0,\n",
        "                \"has_motion\": False\n",
        "            }\n",
        "\n",
        "        # Count frames with people\n",
        "        frames_with_people = 0\n",
        "        face_positions = []\n",
        "        pose_positions = []\n",
        "\n",
        "        # Sample frames if there are many (for efficiency)\n",
        "        if len(frames) > 20:\n",
        "            sample_frames = frames[::len(frames)//20]  # Take ~20 frames evenly distributed\n",
        "        else:\n",
        "            sample_frames = frames\n",
        "\n",
        "        for frame in sample_frames:\n",
        "            # Check for faces\n",
        "            faces = self.face_detector.detect_faces(frame)\n",
        "            # Check for people/poses\n",
        "            people = self.pose_detector.detect_people(frame)\n",
        "\n",
        "            if faces or people:\n",
        "                frames_with_people += 1\n",
        "\n",
        "                # Collect positions for later analysis\n",
        "                for x, y, w, h, _ in faces:\n",
        "                    face_positions.append((x + w//2, y + h//2))\n",
        "\n",
        "                for x, y, _ in people:\n",
        "                    pose_positions.append((x, y))\n",
        "\n",
        "        # Calculate ratio of frames containing people\n",
        "        people_ratio = frames_with_people / len(sample_frames)\n",
        "\n",
        "        # Check for motion in the scene\n",
        "        has_motion = self.detect_scene_motion(sample_frames)\n",
        "\n",
        "        # Determine scene content type and framing strategy\n",
        "        if people_ratio >= self.people_presence_threshold:\n",
        "            # People are present in significant portion of the scene\n",
        "            content_type = \"people\"\n",
        "\n",
        "            # Find most common/important person position\n",
        "            if face_positions or pose_positions:\n",
        "                all_positions = face_positions + pose_positions\n",
        "                if all_positions:\n",
        "                    # Calculate centroid of people positions\n",
        "                    centroid_x = sum(p[0] for p in all_positions) / len(all_positions)\n",
        "                    centroid_y = sum(p[1] for p in all_positions) / len(all_positions)\n",
        "                    framing_strategy = \"track_people\"\n",
        "                    center_of_interest = (int(centroid_x), int(centroid_y))\n",
        "                else:\n",
        "                    framing_strategy = \"center\"\n",
        "                    center_of_interest = self.get_frame_center(frames[0])\n",
        "            else:\n",
        "                framing_strategy = \"center\"\n",
        "                center_of_interest = self.get_frame_center(frames[0])\n",
        "        else:\n",
        "            # Few or no people - analyze visual saliency and motion\n",
        "            content_type = \"no_people\"\n",
        "\n",
        "            if has_motion:\n",
        "                framing_strategy = \"follow_motion\"\n",
        "                center_of_interest = self.find_motion_center(sample_frames)\n",
        "            else:\n",
        "                # Use visual saliency for static scenes without people\n",
        "                framing_strategy = \"visual_saliency\"\n",
        "                center_of_interest = self.find_salient_region(sample_frames)\n",
        "\n",
        "        return {\n",
        "            \"content_type\": content_type,\n",
        "            \"framing_strategy\": framing_strategy,\n",
        "            \"people_ratio\": people_ratio,\n",
        "            \"has_motion\": has_motion,\n",
        "            \"center_of_interest\": center_of_interest\n",
        "        }\n",
        "\n",
        "    def detect_scene_motion(self, frames, threshold=3.0):\n",
        "        \"\"\"Detect if there is significant motion in the scene\"\"\"\n",
        "        if len(frames) < 3:\n",
        "            return False\n",
        "\n",
        "        # Sample frames for efficiency\n",
        "        if len(frames) > 6:\n",
        "            sample_indices = np.linspace(0, len(frames)-1, 6, dtype=int)\n",
        "            sample_frames = [frames[i] for i in sample_indices]\n",
        "        else:\n",
        "            sample_frames = frames\n",
        "\n",
        "        # Convert to grayscale\n",
        "        gray_frames = [cv2.cvtColor(f, cv2.COLOR_BGR2GRAY) for f in sample_frames]\n",
        "\n",
        "        # Calculate motion between consecutive frames\n",
        "        motion_scores = []\n",
        "        for i in range(len(gray_frames)-1):\n",
        "            # Simple motion detection using frame difference\n",
        "            diff = cv2.absdiff(gray_frames[i], gray_frames[i+1])\n",
        "            motion_score = np.mean(diff)\n",
        "            motion_scores.append(motion_score)\n",
        "\n",
        "        avg_motion = np.mean(motion_scores) if motion_scores else 0\n",
        "        return avg_motion > threshold\n",
        "\n",
        "    def find_motion_center(self, frames):\n",
        "        \"\"\"Find the center of motion in a sequence of frames\"\"\"\n",
        "        if len(frames) < 3:\n",
        "            return self.get_frame_center(frames[0])\n",
        "\n",
        "        # Use optical flow to track motion\n",
        "        prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
        "        motion_points = []\n",
        "\n",
        "        for i in range(1, min(len(frames), 10)):  # Process up to 10 frames\n",
        "            gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Calculate optical flow\n",
        "            flow = cv2.calcOpticalFlowFarneback(\n",
        "                prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "\n",
        "            # Get magnitude and angle of flow\n",
        "            mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "\n",
        "            # Find points with significant motion\n",
        "            significant_motion = mag > np.percentile(mag, 90)  # Top 10% of motion\n",
        "            y_indices, x_indices = np.where(significant_motion)\n",
        "\n",
        "            if len(y_indices) > 0:\n",
        "                # Use weighted average based on magnitude\n",
        "                weights = [mag[y, x] for y, x in zip(y_indices, x_indices)]\n",
        "                total_weight = sum(weights)\n",
        "                if total_weight > 0:\n",
        "                    center_x = sum(x * w for x, w in zip(x_indices, weights)) / total_weight\n",
        "                    center_y = sum(y * w for y, w in zip(y_indices, weights)) / total_weight\n",
        "                    motion_points.append((int(center_x), int(center_y)))\n",
        "\n",
        "            prev_gray = gray\n",
        "\n",
        "        if motion_points:\n",
        "            # Calculate weighted average of motion centers\n",
        "            x_avg = sum(p[0] for p in motion_points) / len(motion_points)\n",
        "            y_avg = sum(p[1] for p in motion_points) / len(motion_points)\n",
        "            return (int(x_avg), int(y_avg))\n",
        "        else:\n",
        "            return self.get_frame_center(frames[0])\n",
        "\n",
        "    def find_salient_region(self, frames):\n",
        "        \"\"\"Find visually salient region in frames using saliency detection\"\"\"\n",
        "        saliency_maps = []\n",
        "\n",
        "        # Calculate saliency for each frame\n",
        "        for frame in frames[:min(5, len(frames))]:  # Use up to 5 frames\n",
        "            success, saliency_map = self.saliency.computeSaliency(frame)\n",
        "            if success:\n",
        "                # Normalize to 0-255 range\n",
        "                saliency_map = (saliency_map * 255).astype('uint8')\n",
        "                saliency_maps.append(saliency_map)\n",
        "\n",
        "        if not saliency_maps:\n",
        "            return self.get_frame_center(frames[0])\n",
        "\n",
        "        # Combine saliency maps\n",
        "        combined_saliency = np.mean(saliency_maps, axis=0)\n",
        "\n",
        "        # Find center of most salient region\n",
        "        _, thresh_map = cv2.threshold(\n",
        "            combined_saliency,\n",
        "            0.7 * np.max(combined_saliency),\n",
        "            255,\n",
        "            cv2.THRESH_BINARY\n",
        "        )\n",
        "\n",
        "        # Find connected components\n",
        "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n",
        "            thresh_map.astype(np.uint8),\n",
        "            connectivity=8\n",
        "        )\n",
        "\n",
        "        # Find largest component (excluding background at index 0)\n",
        "        if num_labels > 1:\n",
        "            max_area = 0\n",
        "            max_idx = 0\n",
        "            for i in range(1, num_labels):  # Skip background\n",
        "                area = stats[i, cv2.CC_STAT_AREA]\n",
        "                if area > max_area:\n",
        "                    max_area = area\n",
        "                    max_idx = i\n",
        "\n",
        "            return (int(centroids[max_idx][0]), int(centroids[max_idx][1]))\n",
        "        else:\n",
        "            # Rule of thirds positioning if no clear salient region\n",
        "            h, w = frames[0].shape[:2]\n",
        "            return (int(w * 2/3), int(h * 1/3))  # Upper-right third intersection\n",
        "\n",
        "    def get_frame_center(self, frame):\n",
        "        \"\"\"Get the center point of a frame\"\"\"\n",
        "        h, w = frame.shape[:2]\n",
        "        return (w // 2, h // 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UWgdFgEvxxC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "class SmoothCropper:\n",
        "    \"\"\"\n",
        "    Creates stable, smooth crop windows with cinematic movement.\n",
        "    Handles different aspect ratios while maintaining visual stability.\n",
        "    \"\"\"\n",
        "    def __init__(self, target_aspect_ratio=9/16, transition_frames=30):\n",
        "        self.target_aspect_ratio = target_aspect_ratio\n",
        "        self.transition_frames = transition_frames\n",
        "\n",
        "        # Crop window history\n",
        "        self.crop_history = deque(maxlen=60)  # Store up to 2 seconds at 30fps\n",
        "\n",
        "        # Current crop state\n",
        "        self.current_crop = None\n",
        "        self.target_crop = None\n",
        "\n",
        "        # Transition state\n",
        "        self.in_transition = False\n",
        "        self.transition_progress = 0\n",
        "        self.transition_start_crop = None\n",
        "        self.transition_end_crop = None\n",
        "\n",
        "        # Anti-jitter settings\n",
        "        self.min_movement_threshold = 5  # Pixels\n",
        "        self.hysteresis_ratio = 0.2  # Move only if change is significant\n",
        "\n",
        "        # Content-aware framing options\n",
        "        self.headroom_ratio = 0.1  # Extra space above detected faces\n",
        "\n",
        "    def calculate_crop(self, frame, focus_point, content_type=\"people\"):\n",
        "        \"\"\"\n",
        "        Calculate the optimal crop window centered on the focus point.\n",
        "\n",
        "        Args:\n",
        "            frame: The video frame\n",
        "            focus_point: (x, y) center point to focus on\n",
        "            content_type: Type of content (\"people\", \"motion\", etc.)\n",
        "\n",
        "        Returns:\n",
        "            dict: Crop parameters (left, right, top, bottom)\n",
        "        \"\"\"\n",
        "        h, w = frame.shape[:2]\n",
        "\n",
        "        # Calculate target width for given aspect ratio\n",
        "        target_width = int(h * self.target_aspect_ratio)\n",
        "        if target_width > w:\n",
        "            # Handle case where target width exceeds frame width\n",
        "            target_width = w\n",
        "\n",
        "        # Adjust focus point based on content type\n",
        "        adjusted_focus_x, adjusted_focus_y = focus_point\n",
        "\n",
        "        if content_type == \"people\":\n",
        "            # Add headroom for people (shift focus point down slightly)\n",
        "            headroom = int(h * self.headroom_ratio)\n",
        "            adjusted_focus_y = min(h - 1, adjusted_focus_y + headroom)\n",
        "\n",
        "        # Calculate initial crop boundaries centered on adjusted focus point\n",
        "        half_width = target_width // 2\n",
        "        left = adjusted_focus_x - half_width\n",
        "        right = adjusted_focus_x + half_width\n",
        "\n",
        "        # Adjust if crop goes out of bounds\n",
        "        if left < 0:\n",
        "            left = 0\n",
        "            right = target_width\n",
        "        elif right > w:\n",
        "            right = w\n",
        "            left = max(0, w - target_width)\n",
        "\n",
        "        # Ensure exact width needed for aspect ratio\n",
        "        if right - left != target_width:\n",
        "            right = min(w, left + target_width)\n",
        "\n",
        "        crop = {\n",
        "            \"left\": int(left),\n",
        "            \"right\": int(right),\n",
        "            \"top\": 0,\n",
        "            \"bottom\": h\n",
        "        }\n",
        "\n",
        "        return crop\n",
        "\n",
        "    def apply_smooth_crop(self, frame, focus_point, force_update=False,\n",
        "                         content_type=\"people\", new_shot=False):\n",
        "        \"\"\"\n",
        "        Apply a smoothed crop to the frame based on focus point.\n",
        "\n",
        "        Args:\n",
        "            frame: The video frame\n",
        "            focus_point: (x, y) center point to focus on\n",
        "            force_update: Force update even if movement is small\n",
        "            content_type: Type of content for content-aware framing\n",
        "            new_shot: True if this is the start of a new shot\n",
        "\n",
        "        Returns:\n",
        "            tuple: (cropped_frame, crop_data)\n",
        "        \"\"\"\n",
        "        h, w = frame.shape[:2]\n",
        "\n",
        "        # Calculate target crop based on current focus point\n",
        "        target_crop = self.calculate_crop(frame, focus_point, content_type)\n",
        "\n",
        "        # For first frame or new shot\n",
        "        if self.current_crop is None or new_shot:\n",
        "            self.current_crop = target_crop\n",
        "            self.crop_history.clear()\n",
        "            self.crop_history.append(self.current_crop)\n",
        "\n",
        "            # Apply crop\n",
        "            cropped = frame[:, target_crop[\"left\"]:target_crop[\"right\"]]\n",
        "            return cropped, target_crop\n",
        "\n",
        "        # Determine if we need to transition to a new crop\n",
        "        if self.in_transition:\n",
        "            # Continue existing transition\n",
        "            self.transition_progress += 1\n",
        "            progress_ratio = self.transition_progress / self.transition_frames\n",
        "\n",
        "            if self.transition_progress >= self.transition_frames:\n",
        "                # Transition complete\n",
        "                self.in_transition = False\n",
        "                self.current_crop = self.transition_end_crop\n",
        "            else:\n",
        "                # Apply easing function for smooth transition\n",
        "                t = progress_ratio\n",
        "                # Cubic easing\n",
        "                t = t * t * (3 - 2 * t)\n",
        "\n",
        "                # Interpolate crop values\n",
        "                left = int(self.transition_start_crop[\"left\"] +\n",
        "                          (self.transition_end_crop[\"left\"] - self.transition_start_crop[\"left\"]) * t)\n",
        "                right = int(self.transition_start_crop[\"right\"] +\n",
        "                           (self.transition_end_crop[\"right\"] - self.transition_start_crop[\"right\"]) * t)\n",
        "\n",
        "                self.current_crop = {\n",
        "                    \"left\": left,\n",
        "                    \"right\": right,\n",
        "                    \"top\": 0,\n",
        "                    \"bottom\": h\n",
        "                }\n",
        "        else:\n",
        "            # Check if the target crop is significantly different from current crop\n",
        "            current_center = (self.current_crop[\"left\"] + self.current_crop[\"right\"]) // 2\n",
        "            target_center = (target_crop[\"left\"] + target_crop[\"right\"]) // 2\n",
        "\n",
        "            movement = abs(current_center - target_center)\n",
        "\n",
        "            # Determine if we should start a new transition\n",
        "            if force_update or movement > self.min_movement_threshold:\n",
        "                significant_change = movement > self.hysteresis_ratio * (target_crop[\"right\"] - target_crop[\"left\"])\n",
        "\n",
        "                if significant_change or force_update:\n",
        "                    # Start new transition\n",
        "                    self.in_transition = True\n",
        "                    self.transition_progress = 0\n",
        "                    self.transition_start_crop = self.current_crop.copy()\n",
        "                    self.transition_end_crop = target_crop.copy()\n",
        "\n",
        "        # Add current crop to history\n",
        "        self.crop_history.append(self.current_crop)\n",
        "\n",
        "        # Create cropped frame\n",
        "        cropped = frame[:, self.current_crop[\"left\"]:self.current_crop[\"right\"]]\n",
        "\n",
        "        return cropped, self.current_crop\n",
        "\n",
        "    def long_term_stabilization(self):\n",
        "        \"\"\"\n",
        "        Apply long-term stabilization by analyzing the history of crop windows.\n",
        "        Helps reduce slow drift and maintain more stable framing.\n",
        "\n",
        "        Should be called periodically during stable scenes.\n",
        "        \"\"\"\n",
        "        if len(self.crop_history) < 15:\n",
        "            return\n",
        "\n",
        "        # Extract centers of recent crops\n",
        "        recent_crops = list(self.crop_history)[-15:]\n",
        "        centers = [(c[\"left\"] + c[\"right\"]) // 2 for c in recent_crops]\n",
        "\n",
        "        # Check if centers are relatively stable\n",
        "        center_std = np.std(centers)\n",
        "\n",
        "        if center_std < 10:  # Very stable\n",
        "            # Calculate average center position\n",
        "            avg_center = int(np.mean(centers))\n",
        "            crop_width = self.current_crop[\"right\"] - self.current_crop[\"left\"]\n",
        "\n",
        "            # Create a stabilized crop around this center\n",
        "            stable_left = avg_center - crop_width // 2\n",
        "            stable_right = avg_center + crop_width // 2\n",
        "\n",
        "            # Apply small correction toward stable position\n",
        "            current_center = (self.current_crop[\"left\"] + self.current_crop[\"right\"]) // 2\n",
        "            diff = avg_center - current_center\n",
        "\n",
        "            if abs(diff) > 3:  # Only correct if difference is noticeable\n",
        "                correction = int(diff * 0.2)  # Apply 20% correction\n",
        "                self.current_crop[\"left\"] += correction\n",
        "                self.current_crop[\"right\"] += correction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvL8g6GKKTw0"
      },
      "source": [
        "### **MAIN PIPELINE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnYEhGn3E87q",
        "outputId": "32eb6aa7-03cc-49d6-e4af-b51fcb71adde"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:pyscenedetect:VideoManager is deprecated and will be removed.\n",
            "INFO:pyscenedetect:Loaded 1 video, framerate: 23.976 FPS, resolution: 1920 x 1080\n",
            "INFO:pyscenedetect:Detecting scenes...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected 150 scenes\n",
            "Processing video...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 29/11231 [00:04<28:41,  6.51it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 0 has no people, marking for potential removal\n",
            "\n",
            "Transitioning to scene 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 42/11231 [00:04<09:44, 19.13it/s]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n",
            "  0%|          | 50/11231 [00:25<3:44:14,  1.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 1 has no people, marking for potential removal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 90/11231 [00:26<15:06, 12.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 122/11231 [00:28<10:44, 17.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|▏         | 154/11231 [00:33<24:06,  7.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 176/11231 [00:36<27:17,  6.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 241/11231 [00:42<09:00, 20.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 331/11231 [00:51<15:45, 11.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 382/11231 [00:56<12:42, 14.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 463/11231 [01:00<04:40, 38.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 530/11231 [01:04<06:13, 28.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 554/11231 [01:08<40:51,  4.35it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 581/11231 [01:31<5:07:26,  1.73s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 11 has no people, marking for potential removal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 611/11231 [01:31<27:17,  6.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 694/11231 [01:39<15:26, 11.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 746/11231 [01:45<11:45, 14.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  7%|▋         | 792/11231 [01:50<08:17, 20.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 952/11231 [02:00<10:15, 16.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 1006/11231 [02:05<11:31, 14.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|▉         | 1072/11231 [02:13<11:10, 15.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 1220/11231 [02:25<11:20, 14.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 1255/11231 [02:29<15:41, 10.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█▏        | 1271/11231 [02:31<15:54, 10.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 1299/11231 [02:54<4:31:16,  1.64s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 21 has no people, marking for potential removal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 1327/11231 [02:54<25:55,  6.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 22\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 1403/11231 [03:02<10:55, 14.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 1451/11231 [03:07<11:33, 14.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 1478/11231 [03:10<22:34,  7.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 1507/11231 [03:12<15:55, 10.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 26\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 1549/11231 [03:17<09:52, 16.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 27\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 1585/11231 [03:20<12:38, 12.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 28\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▍        | 1647/11231 [03:23<04:36, 34.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 1697/11231 [03:29<10:59, 14.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 1792/11231 [03:35<04:05, 38.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 1880/11231 [03:41<13:19, 11.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 1906/11231 [03:45<26:57,  5.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 1950/11231 [03:51<12:58, 11.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▉        | 2136/11231 [04:00<08:45, 17.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2270/11231 [04:13<06:56, 21.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██▏       | 2388/11231 [04:19<04:55, 29.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 37\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 2455/11231 [04:27<15:52,  9.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 38\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 2554/11231 [04:35<08:14, 17.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 39\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▍       | 2756/11231 [04:55<11:23, 12.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 40\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 25%|██▌       | 2830/11231 [05:00<06:11, 22.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 41\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 2883/11231 [05:05<06:23, 21.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▋       | 2952/11231 [05:09<10:50, 12.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 43\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 3101/11231 [05:18<06:25, 21.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 44\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 3130/11231 [05:20<09:49, 13.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 3233/11231 [05:26<04:19, 30.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▉       | 3307/11231 [05:33<06:18, 20.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 47\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|██▉       | 3358/11231 [05:37<04:48, 27.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 48\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 3445/11231 [05:46<07:00, 18.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 49\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 3477/11231 [05:49<10:02, 12.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 3541/11231 [05:55<04:38, 27.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 3570/11231 [05:57<10:05, 12.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 3613/11231 [06:05<09:32, 13.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 53\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 3692/11231 [06:12<08:31, 14.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 54\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 3742/11231 [06:17<05:52, 21.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 55\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▎      | 3778/11231 [06:21<08:43, 14.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 56\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 3837/11231 [06:24<04:13, 29.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 3884/11231 [06:29<09:47, 12.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▌      | 3942/11231 [06:37<11:12, 10.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 3997/11231 [06:44<08:13, 14.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 60\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 4050/11231 [06:51<12:32,  9.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 4220/11231 [07:05<07:34, 15.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 4266/11231 [07:11<06:40, 17.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 63\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▊      | 4332/11231 [07:18<07:34, 15.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 4364/11231 [07:20<07:38, 14.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 65\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 4408/11231 [07:28<09:48, 11.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 66\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 4423/11231 [07:28<05:04, 22.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 67\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 4443/11231 [07:30<12:00,  9.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 68\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4513/11231 [07:38<07:52, 14.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 4599/11231 [07:48<09:10, 12.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 70\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 4723/11231 [08:00<10:08, 10.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 71\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 4781/11231 [08:09<07:28, 14.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 72\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 4808/11231 [08:32<3:33:26,  1.99s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 72 has no people, marking for potential removal\n",
            "\n",
            "Transitioning to scene 73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 4839/11231 [08:54<1:41:25,  1.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 73 has no people, marking for potential removal\n",
            "\n",
            "Transitioning to scene 74\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 4879/11231 [08:58<21:28,  4.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 75\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▎     | 4908/11231 [09:21<3:01:24,  1.72s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 75 has no people, marking for potential removal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 4958/11231 [09:22<06:13, 16.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 4990/11231 [09:24<07:21, 14.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 77\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 5018/11231 [09:50<3:14:51,  1.88s/it] "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scene 77 has no people, marking for potential removal\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 5062/11231 [09:51<08:40, 11.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 78\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 45%|████▌     | 5097/11231 [09:54<08:21, 12.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 79\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 5163/11231 [10:03<05:10, 19.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 80\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 5237/11231 [10:09<11:34,  8.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 81\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 5368/11231 [10:21<06:54, 14.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 5397/11231 [10:24<11:16,  8.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 5486/11231 [10:32<05:05, 18.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 84\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 5530/11231 [10:36<04:35, 20.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 85\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 5713/11231 [10:51<02:55, 31.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 86\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████▏    | 5768/11231 [10:55<07:28, 12.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 87\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 5803/11231 [10:58<06:16, 14.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 88\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 5823/11231 [11:00<13:33,  6.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 5905/11231 [11:08<04:03, 21.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 90\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 5933/11231 [11:10<06:37, 13.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 91\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 5991/11231 [11:17<04:29, 19.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 92\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▎    | 6030/11231 [11:20<03:55, 22.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 93\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 6087/11231 [11:24<05:05, 16.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 94\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 6180/11231 [11:34<05:35, 15.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 95\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 55%|█████▌    | 6229/11231 [11:39<05:35, 14.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 96\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 6347/11231 [11:50<04:12, 19.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 97\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 6389/11231 [11:53<04:28, 18.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 98\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 6519/11231 [12:05<03:53, 20.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 99\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 6540/11231 [12:07<11:51,  6.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 59%|█████▉    | 6651/11231 [12:17<05:33, 13.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 6805/11231 [12:31<05:11, 14.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████▏   | 6880/11231 [12:36<01:54, 38.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 7042/11231 [12:47<05:19, 13.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 104\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 7093/11231 [12:50<05:29, 12.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 105\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 7161/11231 [12:58<03:47, 17.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 106\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 7227/11231 [13:01<02:01, 32.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 7423/11231 [13:08<01:36, 39.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 108\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 7517/11231 [13:12<02:29, 24.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 109\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 7573/11231 [13:15<01:57, 31.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 110\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 7634/11231 [13:18<01:45, 34.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 111\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 69%|██████▉   | 7783/11231 [13:23<01:30, 37.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7913/11231 [13:29<01:22, 40.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 7944/11231 [13:31<03:23, 16.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 114\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 8158/11231 [13:44<01:33, 32.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 115\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 8291/11231 [13:49<01:11, 41.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 8335/11231 [13:52<02:31, 19.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 75%|███████▍  | 8375/11231 [13:55<02:23, 19.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 118\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 8483/11231 [14:04<02:34, 17.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 8558/11231 [14:09<01:25, 31.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▋  | 8591/11231 [14:11<02:32, 17.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 121\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 8763/11231 [14:17<01:30, 27.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 122\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 8878/11231 [14:22<01:02, 37.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 123\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8994/11231 [14:26<00:57, 38.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 124\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 9017/11231 [14:28<03:57,  9.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 9087/11231 [14:38<01:49, 19.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 9224/11231 [14:44<02:37, 12.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 127\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 83%|████████▎ | 9349/11231 [14:57<03:47,  8.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 128\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 9430/11231 [15:05<01:23, 21.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 85%|████████▍ | 9512/11231 [15:09<01:05, 26.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 130\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 9653/11231 [15:22<02:02, 12.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 131\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▋ | 9711/11231 [15:25<00:47, 31.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 132\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 9858/11231 [15:30<00:34, 39.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 133\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 9916/11231 [15:33<00:36, 35.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 134\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|████████▉ | 10059/11231 [15:46<00:57, 20.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 135\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 10132/11231 [15:50<00:31, 35.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 136\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 10171/11231 [15:53<00:49, 21.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 137\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 10236/11231 [15:57<00:39, 24.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 10282/11231 [15:59<00:35, 26.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 139\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 93%|█████████▎| 10467/11231 [16:06<00:19, 38.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 140\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▎| 10514/11231 [16:09<00:26, 26.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▍| 10640/11231 [16:16<00:24, 23.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 142\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 10851/11231 [16:23<00:12, 30.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 10966/11231 [16:34<00:12, 20.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 11001/11231 [16:37<00:12, 18.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 145\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 11053/11231 [16:39<00:05, 30.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 146\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 11103/11231 [16:45<00:06, 18.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 11172/11231 [16:49<00:02, 28.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 148\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 11203/11231 [16:51<00:02, 13.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 149\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 11231/11231 [16:55<00:00, 11.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Transitioning to scene 150\n",
            "Adding audio track to the processed video...\n",
            "Successfully added audio to: output_vertical_video.mp4\n",
            "Processing complete. Output saved to output_vertical_video.mp4\n",
            "Focus point data saved to output_vertical_video_tracking.json\n",
            "Tracking data saved to: output_vertical_video_tracking.json\n"
          ]
        }
      ],
      "source": [
        "# MAIN\n",
        "import cv2\n",
        "import numpy as np\n",
        "import scenedetect\n",
        "from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "import mediapipe as mp\n",
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "import time\n",
        "#from virtual_camera import VirtualCamera\n",
        "#from scene_analyzer import SceneContentAnalyzer\n",
        "#from smooth_cropper import SmoothCropper\n",
        "\n",
        "\n",
        "class ImprovedH2VProcessor:\n",
        "    def __init__(self):\n",
        "        # Initialize MediaPipe solutions\n",
        "        self.mp_face_detection = mp.solutions.face_detection\n",
        "        self.mp_face_detection_model = self.mp_face_detection.FaceDetection(min_detection_confidence=0.5)\n",
        "        self.mp_pose = mp.solutions.pose\n",
        "        self.mp_pose_model = self.mp_pose.Pose(min_detection_confidence=0.5,\n",
        "                                              min_tracking_confidence=0.5)\n",
        "\n",
        "        # Initialize advanced components\n",
        "        self.virtual_camera = VirtualCamera()\n",
        "        self.scene_analyzer = SceneContentAnalyzer(self, self)  # Pass self as face/pose detector\n",
        "        self.smooth_cropper = SmoothCropper(target_aspect_ratio=9/16)\n",
        "\n",
        "        # Scene tracking\n",
        "        self.current_shot_id = 0\n",
        "        self.shot_boundaries = []\n",
        "        self.current_scene_frames = []\n",
        "        self.current_scene_analysis = None\n",
        "        self.scenes_without_people = []\n",
        "\n",
        "        # Tracking data\n",
        "        self.focus_points_data = []\n",
        "        self.prev_frame = None\n",
        "\n",
        "        # Stabilization history\n",
        "        self.prev_focus_point = None\n",
        "        self.focus_point_history = collections.deque(maxlen=30)\n",
        "\n",
        "        # Enhanced stability parameters\n",
        "        self.temporal_smoothing_window = 15  # Frames for temporal averaging\n",
        "        self.min_confidence_threshold = 0.7  # Minimum confidence for detection\n",
        "        self.static_scene_counter = 0  # Counter for static scenes\n",
        "        self.is_static_scene = False\n",
        "        self.static_scene_focus_point = None\n",
        "\n",
        "    def detect_scenes(self, video_path):\n",
        "        \"\"\"Split video into scenes using PySceneDetect with higher threshold for stability\"\"\"\n",
        "        video_manager = VideoManager([video_path])\n",
        "        scene_manager = SceneManager()\n",
        "        # Higher threshold means fewer scene changes, more stability\n",
        "        scene_manager.add_detector(ContentDetector(threshold=40))\n",
        "\n",
        "        video_manager.start()\n",
        "        scene_manager.detect_scenes(frame_source=video_manager)\n",
        "\n",
        "        scene_list = scene_manager.get_scene_list()\n",
        "        # Store scene boundaries for reference during processing\n",
        "        self.shot_boundaries = [(scene[0].frame_num, scene[1].frame_num) for scene in scene_list]\n",
        "        return scene_list\n",
        "\n",
        "    def detect_faces(self, frame):\n",
        "        \"\"\"Detect faces in the frame\"\"\"\n",
        "        results = self.mp_face_detection_model.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        faces = []\n",
        "        if results.detections:\n",
        "            for detection in results.detections:\n",
        "                bboxC = detection.location_data.relative_bounding_box\n",
        "                ih, iw, _ = frame.shape\n",
        "                x, y, w, h = int(bboxC.xmin * iw), int(bboxC.ymin * ih), \\\n",
        "                             int(bboxC.width * iw), int(bboxC.height * ih)\n",
        "                # Add confidence score to help with stability\n",
        "                confidence = detection.score[0]\n",
        "                faces.append((x, y, w, h, confidence))\n",
        "        return faces\n",
        "\n",
        "    def detect_people(self, frame):\n",
        "        \"\"\"Detect people using pose estimation\"\"\"\n",
        "        results = self.mp_pose_model.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        people = []\n",
        "        if results.pose_landmarks:\n",
        "            h, w, _ = frame.shape\n",
        "            # Get upper body landmarks for better stability\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            # Check if key points are visible (nose, shoulders, hips)\n",
        "            key_points = [0, 11, 12, 23, 24]  # Indices for nose, shoulders, and hips\n",
        "            visible_confidence = sum(landmarks[i].visibility for i in key_points) / len(key_points)\n",
        "\n",
        "            if visible_confidence > 0.7:  # Only track if confident\n",
        "                # Calculate center of mass from upper body landmarks for stability\n",
        "                upper_body_x = np.mean([landmarks[i].x for i in key_points if landmarks[i].visibility > 0.5]) * w\n",
        "                upper_body_y = np.mean([landmarks[i].y for i in key_points if landmarks[i].visibility > 0.5]) * h\n",
        "\n",
        "                people.append((int(upper_body_x), int(upper_body_y), visible_confidence))\n",
        "\n",
        "        return people\n",
        "\n",
        "    def detect_motion(self, prev_frame, curr_frame):\n",
        "        \"\"\"Detect significant motion between frames using optical flow\"\"\"\n",
        "        if prev_frame is None:\n",
        "            return None\n",
        "\n",
        "        # Convert frames to grayscale\n",
        "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Calculate sparse optical flow using Lucas-Kanade method\n",
        "        # This is more stable than dense optical flow\n",
        "        feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
        "        prev_points = cv2.goodFeaturesToTrack(prev_gray, mask=None, **feature_params)\n",
        "\n",
        "        if prev_points is None or len(prev_points) == 0:\n",
        "            return None\n",
        "\n",
        "        # Use Lucas-Kanade optical flow\n",
        "        lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
        "                       criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
        "\n",
        "        next_points, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_points, None, **lk_params)\n",
        "\n",
        "        if next_points is None:\n",
        "            return None\n",
        "\n",
        "        # Select good points\n",
        "        good_new = next_points[status == 1]\n",
        "        good_old = prev_points[status == 1]\n",
        "\n",
        "        if len(good_new) == 0:\n",
        "            return None\n",
        "\n",
        "        # Calculate the movement vectors\n",
        "        movements = good_new - good_old\n",
        "\n",
        "        # Calculate magnitudes for each point\n",
        "        magnitudes = np.sqrt(np.sum(movements**2, axis=1))\n",
        "\n",
        "        # If no significant motion, return None\n",
        "        if np.max(magnitudes) < 5:  # Threshold for significant motion\n",
        "            return None\n",
        "\n",
        "        # Find the point with maximum movement\n",
        "        max_idx = np.argmax(magnitudes)\n",
        "        motion_center = (int(good_new[max_idx][0][0]), int(good_new[max_idx][0][1]))\n",
        "\n",
        "        return motion_center\n",
        "\n",
        "    def _get_weighted_focus_point(self, frame, is_new_shot=False):\n",
        "        \"\"\"\n",
        "        Calculate focus point using weighted detection results\n",
        "        with temporal smoothing for stability\n",
        "        \"\"\"\n",
        "        h, w, _ = frame.shape\n",
        "        frame_center = (w // 2, h // 2)\n",
        "\n",
        "        # Reset for new shots\n",
        "        if is_new_shot:\n",
        "            self.focus_point_history.clear()\n",
        "            self.prev_focus_point = None\n",
        "            self.is_static_scene = False\n",
        "            self.static_scene_counter = 0\n",
        "            self.static_scene_focus_point = None\n",
        "            # Return frame center for first frame of a new shot\n",
        "            return frame_center\n",
        "\n",
        "        # Detect faces with higher priority\n",
        "        faces = self.detect_faces(frame)\n",
        "        faces = [f for f in faces if f[4] > self.min_confidence_threshold]  # Filter by confidence\n",
        "\n",
        "        # If faces found, use them as primary focus\n",
        "        if faces:\n",
        "            # Use the largest/most confident face\n",
        "            face_centers = [(f[0] + f[2]//2, f[1] + f[3]//2, f[4]) for f in faces]\n",
        "\n",
        "            # Sort by confidence\n",
        "            face_centers.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "            # If multiple faces, use weighted average of top faces\n",
        "            if len(face_centers) > 1:\n",
        "                # Use top 2 faces only to avoid jumping between many faces\n",
        "                top_faces = face_centers[:2]\n",
        "                total_weight = sum(face[2] for face in top_faces)\n",
        "                focus_x = sum(face[0] * face[2] for face in top_faces) / total_weight\n",
        "                focus_y = sum(face[1] * face[2] for face in top_faces) / total_weight\n",
        "                focus_point = (int(focus_x), int(focus_y))\n",
        "            else:\n",
        "                focus_point = (face_centers[0][0], face_centers[0][1])\n",
        "\n",
        "            # Add to history with high confidence\n",
        "            focus_confidence = 1.0\n",
        "\n",
        "        else:\n",
        "            # No faces, try pose detection\n",
        "            people = self.detect_people(frame)\n",
        "\n",
        "            if people:\n",
        "                # Sort by confidence\n",
        "                people.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "                if len(people) > 1:\n",
        "                    # Weighted average of top 2 people\n",
        "                    top_people = people[:2]\n",
        "                    total_weight = sum(person[2] for person in top_people)\n",
        "                    focus_x = sum(person[0] * person[2] for person in top_people) / total_weight\n",
        "                    focus_y = sum(person[1] * person[2] for person in top_people) / total_weight\n",
        "                    focus_point = (int(focus_x), int(focus_y))\n",
        "                else:\n",
        "                    focus_point = (people[0][0], people[0][1])\n",
        "\n",
        "                # Add to history with medium confidence\n",
        "                focus_confidence = 0.8\n",
        "\n",
        "            else:\n",
        "                # No people, try motion detection only if not already in static scene mode\n",
        "                if not self.is_static_scene and self.prev_frame is not None:\n",
        "                    motion_center = self.detect_motion(self.prev_frame, frame)\n",
        "\n",
        "                    if motion_center:\n",
        "                        focus_point = motion_center\n",
        "                        # Motion is less reliable, use lower confidence\n",
        "                        focus_confidence = 0.6\n",
        "                        # Reset static scene counter when motion detected\n",
        "                        self.static_scene_counter = 0\n",
        "                        self.is_static_scene = False\n",
        "                    else:\n",
        "                        # No motion detected, increment static counter\n",
        "                        self.static_scene_counter += 1\n",
        "\n",
        "                        # If static for multiple frames, switch to static scene mode\n",
        "                        if self.static_scene_counter > 30:  # ~1 second at 30fps\n",
        "                            self.is_static_scene = True\n",
        "\n",
        "                            # Use scene analysis to find compositional focus point\n",
        "                            if not self.static_scene_focus_point:\n",
        "                                # Analyze frame for visual saliency\n",
        "                                self.static_scene_focus_point = self.scene_analyzer.find_salient_region([frame])\n",
        "\n",
        "                            focus_point = self.static_scene_focus_point\n",
        "                            focus_confidence = 0.7  # Medium-high confidence once established\n",
        "                        elif self.prev_focus_point:\n",
        "                            # Use previous focus point with decay\n",
        "                            focus_point = self.prev_focus_point\n",
        "                            focus_confidence = 0.5  # Lower confidence for continued use\n",
        "                        else:\n",
        "                            # Fallback to center\n",
        "                            focus_point = frame_center\n",
        "                            focus_confidence = 0.3  # Low confidence\n",
        "                else:\n",
        "                    # Already in static scene mode or no previous frame\n",
        "                    if self.is_static_scene and self.static_scene_focus_point:\n",
        "                        focus_point = self.static_scene_focus_point\n",
        "                        focus_confidence = 0.7\n",
        "                    elif self.prev_focus_point:\n",
        "                        focus_point = self.prev_focus_point\n",
        "                        focus_confidence = 0.5\n",
        "                    else:\n",
        "                        focus_point = frame_center\n",
        "                        focus_confidence = 0.3\n",
        "\n",
        "        # Apply temporal smoothing for stability\n",
        "        if self.prev_focus_point and not is_new_shot:\n",
        "            # Add confidence-weighted current detection to history\n",
        "            self.focus_point_history.append((focus_point, focus_confidence))\n",
        "\n",
        "            # Calculate temporally smoothed position using weighted average\n",
        "            # Weight more recent frames higher for responsiveness\n",
        "            if len(self.focus_point_history) >= 3:\n",
        "                total_weight = 0\n",
        "                weighted_x = 0\n",
        "                weighted_y = 0\n",
        "\n",
        "                # Apply exponential weighting to favor more recent points\n",
        "                for i, (point, conf) in enumerate(self.focus_point_history):\n",
        "                    # More recent points get exponentially higher weights\n",
        "                    recency_weight = np.exp(i / 10)  # Exponential growth factor\n",
        "                    weight = conf * recency_weight\n",
        "                    weighted_x += point[0] * weight\n",
        "                    weighted_y += point[1] * weight\n",
        "                    total_weight += weight\n",
        "\n",
        "                smoothed_x = int(weighted_x / total_weight)\n",
        "                smoothed_y = int(weighted_y / total_weight)\n",
        "                smoothed_focus = (smoothed_x, smoothed_y)\n",
        "\n",
        "                # Additional hysteresis: Move only part way to new position\n",
        "                if self.prev_focus_point:\n",
        "                    # Calculate distance\n",
        "                    dist = np.sqrt((smoothed_focus[0] - self.prev_focus_point[0])**2 +\n",
        "                                 (smoothed_focus[1] - self.prev_focus_point[1])**2)\n",
        "\n",
        "                    # Apply stronger smoothing for small movements\n",
        "                    if dist < 20:  # Small movement\n",
        "                        # Move only 30% of the way to new position for small changes\n",
        "                        lerp_factor = 0.3\n",
        "                    elif dist < 50:  # Medium movement\n",
        "                        # Move 50% of the way\n",
        "                        lerp_factor = 0.5\n",
        "                    else:  # Large movement - likely intentional\n",
        "                        # Move 70% of the way\n",
        "                        lerp_factor = 0.7\n",
        "\n",
        "                    final_x = int(self.prev_focus_point[0] + lerp_factor * (smoothed_focus[0] - self.prev_focus_point[0]))\n",
        "                    final_y = int(self.prev_focus_point[1] + lerp_factor * (smoothed_focus[1] - self.prev_focus_point[1]))\n",
        "                    smoothed_focus = (final_x, final_y)\n",
        "            else:\n",
        "                smoothed_focus = focus_point\n",
        "        else:\n",
        "            smoothed_focus = focus_point\n",
        "            self.focus_point_history.append((focus_point, focus_confidence))\n",
        "\n",
        "        # Update previous focus point\n",
        "        self.prev_focus_point = smoothed_focus\n",
        "        return smoothed_focus\n",
        "\n",
        "    def process_video(self, input_path, output_path, output_data_path=None):\n",
        "        \"\"\"\n",
        "        Process video using optimized H2V algorithm with improved stability.\n",
        "\n",
        "        Args:\n",
        "            input_path: Path to input horizontal video\n",
        "            output_path: Path to output vertical video\n",
        "            output_data_path: Path to save focus point data (optional)\n",
        "        \"\"\"\n",
        "        # Detect scenes first\n",
        "        scenes = self.detect_scenes(input_path)\n",
        "        print(f\"Detected {len(scenes)} scenes\")\n",
        "\n",
        "        # Open input video\n",
        "        cap = cv2.VideoCapture(input_path)\n",
        "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "        # Prepare output video\n",
        "        output_height = height\n",
        "        output_width = int(height * 9/16)  # 9:16 aspect ratio\n",
        "\n",
        "        # Use lossless intermediate codec\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n",
        "        temp_output_path = output_path + \".temp.avi\"\n",
        "        out = cv2.VideoWriter(temp_output_path, fourcc, fps, (output_width, output_height))\n",
        "\n",
        "        # Track current scene\n",
        "        current_scene_idx = 0\n",
        "        current_frame_idx = 0\n",
        "        buffer_frames = []  # For analyzing scene content before processing\n",
        "        scene_buffer_size = min(30, int(fps))  # Buffer up to 1 second or 30 frames\n",
        "        is_new_shot = True\n",
        "        is_people_scene = True\n",
        "\n",
        "        print(\"Processing video...\")\n",
        "        progress_bar = tqdm(total=total_frames)\n",
        "\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            current_frame_idx += 1\n",
        "            progress_bar.update(1)\n",
        "\n",
        "            # Check if we're entering a new scene\n",
        "            if (current_scene_idx < len(scenes) and\n",
        "                current_frame_idx >= scenes[current_scene_idx][1].frame_num):\n",
        "                current_scene_idx += 1\n",
        "                is_new_shot = True\n",
        "                buffer_frames = []\n",
        "\n",
        "                # Reset stabilization\n",
        "                self.virtual_camera = VirtualCamera()\n",
        "                self.focus_point_history.clear()\n",
        "                self.prev_focus_point = None\n",
        "                self.is_static_scene = False\n",
        "\n",
        "                # Print scene transition\n",
        "                print(f\"\\nTransitioning to scene {current_scene_idx}\")\n",
        "\n",
        "            # Buffer frames for scene analysis\n",
        "            if len(buffer_frames) < scene_buffer_size:\n",
        "                buffer_frames.append(frame.copy())\n",
        "\n",
        "                # If we have enough frames, analyze scene content\n",
        "                if len(buffer_frames) == scene_buffer_size:\n",
        "                    scene_analysis = self.scene_analyzer.analyze_scene_frames(buffer_frames)\n",
        "                    is_people_scene = scene_analysis[\"content_type\"] == \"people\"\n",
        "\n",
        "                    # Skip scenes without people if requested\n",
        "                    if not is_people_scene:\n",
        "                        print(f\"Scene {current_scene_idx} has no people, marking for potential removal\")\n",
        "                        self.scenes_without_people.append(current_scene_idx)\n",
        "\n",
        "            # Store previous frame for motion detection\n",
        "            self.prev_frame = frame.copy()\n",
        "\n",
        "            # Skip processing for scenes without people if desired\n",
        "            # This is where you'd implement your second objective\n",
        "            if not is_people_scene:\n",
        "                # Option 1: Skip frame entirely (will make video shorter)\n",
        "                # continue\n",
        "\n",
        "                # Option 2: Use center crop for scenes without people\n",
        "                focus_point = (width // 2, height // 2)\n",
        "                is_new_shot = False\n",
        "            else:\n",
        "                # Get focus point with enhanced stability\n",
        "                focus_point = self._get_weighted_focus_point(frame, is_new_shot)\n",
        "\n",
        "            # Use virtual camera to smooth movement and reduce jitter\n",
        "            camera_center = self.virtual_camera.update(focus_point, force_immediate=is_new_shot)\n",
        "\n",
        "            # Apply smooth cropping\n",
        "            cropped_frame, crop_data = self.smooth_cropper.apply_smooth_crop(\n",
        "                frame, camera_center, force_update=is_new_shot,\n",
        "                content_type=\"people\" if is_people_scene else \"other\",\n",
        "                new_shot=is_new_shot\n",
        "            )\n",
        "\n",
        "            # Reset new shot flag\n",
        "            if is_new_shot:\n",
        "                is_new_shot = False\n",
        "\n",
        "            # Apply long-term stabilization occasionally during stable scenes\n",
        "            if current_frame_idx % 30 == 0 and not is_new_shot:\n",
        "                self.smooth_cropper.long_term_stabilization()\n",
        "\n",
        "            # Resize to target output resolution if needed\n",
        "            if cropped_frame.shape[1] != output_width or cropped_frame.shape[0] != output_height:\n",
        "                cropped_frame = cv2.resize(cropped_frame, (output_width, output_height))\n",
        "\n",
        "            # Store focus point data\n",
        "            self.focus_points_data.append({\n",
        "                \"frame\": current_frame_idx,\n",
        "                \"scene\": current_scene_idx,\n",
        "                \"timestamp\": current_frame_idx / fps,\n",
        "                \"focus_point\": {\"x\": camera_center[0], \"y\": camera_center[1]},\n",
        "                \"crop\": {\n",
        "                    \"left\": crop_data[\"left\"],\n",
        "                    \"right\": crop_data[\"right\"],\n",
        "                    \"top\": crop_data[\"top\"],\n",
        "                    \"bottom\": crop_data[\"bottom\"]\n",
        "                }\n",
        "            })\n",
        "\n",
        "            # Write frame\n",
        "            out.write(cropped_frame)\n",
        "\n",
        "        # Release resources\n",
        "        cap.release()\n",
        "        out.release()\n",
        "        progress_bar.close()\n",
        "\n",
        "        # Convert to H.264 with appropriate bitrate\n",
        "        #output_bitrate = \"5M\"  # Adjust as needed for quality\n",
        "        #self._convert_to_mp4(temp_output_path, output_path, output_bitrate)\n",
        "\n",
        "        # Combine the processed video with the original audio using FFmpeg\n",
        "        self.combine_video_with_audio(input_path, temp_output_path, output_path)\n",
        "\n",
        "        # Clean up temp file\n",
        "        if os.path.exists(temp_output_path):\n",
        "            os.remove(temp_output_path)\n",
        "\n",
        "        # Save focus point data if requested\n",
        "        if output_data_path:\n",
        "            with open(output_data_path, 'w') as f:\n",
        "                json.dump({\n",
        "                    \"focus_points\": self.focus_points_data,\n",
        "                    \"scenes_without_people\": self.scenes_without_people\n",
        "                }, f, indent=2)\n",
        "\n",
        "        print(f\"Processing complete. Output saved to {output_path}\")\n",
        "        if output_data_path:\n",
        "            print(f\"Focus point data saved to {output_data_path}\")\n",
        "\n",
        "        return output_data_path\n",
        "\n",
        "    def _convert_to_mp4(self, input_path, output_path, bitrate=\"5M\"):\n",
        "        \"\"\"Convert video to mp4 with specified bitrate using ffmpeg\"\"\"\n",
        "        cmd = [\n",
        "            \"ffmpeg\", \"-i\", input_path,\n",
        "            \"-c:v\", \"libx264\", \"-preset\", \"slow\",\n",
        "            \"-b:v\", bitrate, \"-maxrate\", bitrate, \"-bufsize\", bitrate,\n",
        "            \"-pix_fmt\", \"yuv420p\", \"-y\", output_path\n",
        "        ]\n",
        "        subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "\n",
        "    def combine_video_with_audio(self, original_video, processed_video, output_path):\n",
        "        \"\"\"Combine the processed video with the original audio using FFmpeg\"\"\"\n",
        "        try:\n",
        "            print(\"Adding audio track to the processed video...\")\n",
        "            # Command to extract audio from original video and combine with new video\n",
        "            ffmpeg_cmd = [\n",
        "                'ffmpeg',\n",
        "                '-i', processed_video,  # Input processed video (no audio)\n",
        "                '-i', original_video,   # Input original video (for audio)\n",
        "                '-c:v', 'copy',         # Copy video stream without re-encoding\n",
        "                '-c:a', 'aac',          # Use AAC codec for audio\n",
        "                '-map', '0:v:0',        # Use video from first input\n",
        "                '-map', '1:a:0',        # Use audio from second input\n",
        "                '-shortest',            # Finish encoding when the shortest input stream ends\n",
        "                '-y',                   # Overwrite output file if it exists\n",
        "                output_path\n",
        "            ]\n",
        "\n",
        "            # Run the FFmpeg command\n",
        "            subprocess.run(ffmpeg_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            print(f\"Successfully added audio to: {output_path}\")\n",
        "\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error adding audio: {e}\")\n",
        "            print(f\"FFmpeg stderr: {e.stderr.decode() if e.stderr else 'No error output'}\")\n",
        "            # If FFmpeg fails, just use the video without audio\n",
        "            import shutil\n",
        "            shutil.copy(processed_video, output_path)\n",
        "            print(f\"Using video without audio: {output_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Unexpected error adding audio: {e}\")\n",
        "            # If another error occurs, just use the video without audio\n",
        "            import shutil\n",
        "            shutil.copy(processed_video, output_path)\n",
        "            print(f\"Using video without audio: {output_path}\")\n",
        "\n",
        "\n",
        "    def process_batch(self, input_dir, output_dir, threads=1):\n",
        "        \"\"\"Process multiple videos in batch mode with optional multithreading\"\"\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        video_files = [f for f in os.listdir(input_dir)\n",
        "                      if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
        "\n",
        "        print(f\"Found {len(video_files)} videos to process\")\n",
        "\n",
        "        if threads > 1:\n",
        "            # TODO: Implement parallel processing if needed\n",
        "            pass\n",
        "        else:\n",
        "            for video_file in video_files:\n",
        "                input_path = os.path.join(input_dir, video_file)\n",
        "                output_name = f\"{os.path.splitext(video_file)[0]}_vertical.mp4\"\n",
        "                output_path = os.path.join(output_dir, output_name)\n",
        "                output_data_path = os.path.join(output_dir, f\"{os.path.splitext(video_file)[0]}_data.json\")\n",
        "\n",
        "                print(f\"Processing {video_file}...\")\n",
        "                self.process_video(input_path, output_path, output_data_path)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    processor = ImprovedH2VProcessor()\n",
        "\n",
        "    input_video = \"input_video.mp4\"  # Replace with your video path\n",
        "    output_video = \"output_vertical_video.mp4\"\n",
        "    # Export focus points data as JSON\n",
        "    #tracking_data_path = os.path.splitext(output_video)[0] + \"_tracking.json\"\n",
        "    tracking_data = 'output_vertical_video_tracking.json'\n",
        "    focus_points_tracking_data = processor.process_video(input_video, output_video, tracking_data)\n",
        "    #print(f\"Processing complete. Vertical video saved to: {video_path}\")\n",
        "    print(f\"Tracking data saved to: {focus_points_tracking_data }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guXwHjNT35KF"
      },
      "source": [
        "### **Play Portrait Video**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SdQdrqHroGKh",
        "outputId": "fe0d5b19-63a4-4703-b6bf-93fb2bebb87f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "# Replace 'your_video.mp4' with the actual path to your video file\n",
        "mp4 = open('output_vertical_video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w80pTEq4A6r"
      },
      "source": [
        "### **Download Portrait Video and Tracking file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PErepH5C2WXZ",
        "outputId": "1afb7023-6bfc-40ee-85a4-ee7f85f780d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4600d126-0842-4d9b-913c-68134274ef75\", \"output_vertical_video.mp4\", 375195874)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_5bb9fad7-fb75-4eff-b74d-4bdf22ce3d25\", \"output_vertical_video_tracking.json\", 2996302)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import scenedetect\n",
        "from scenedetect import VideoManager, SceneManager\n",
        "from scenedetect.detectors import ContentDetector\n",
        "import mediapipe as mp\n",
        "import json\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "import collections\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "\n",
        "# Replace 'your_video.mp4' with the actual path to your video file\n",
        "mp4 = open('output_vertical_video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)\n",
        "\n",
        "# Download the video file\n",
        "files.download('output_vertical_video.mp4')\n",
        "\n",
        "# Assuming your tracking data is in a file named 'tracking_data.json'\n",
        "# Replace 'tracking_data.json' with the actual file name if different\n",
        "try:\n",
        "    files.download('output_vertical_video_tracking.json')\n",
        "except FileNotFoundError:\n",
        "    print(\"tracking_data.json not found. Please make sure the file exists.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3xrjgnZJ7Oi"
      },
      "source": [
        "### **Add Portrait Video to Google Drive Folder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "SsnSkweBGoBM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "f1f16bc5-d102-47a9-90fc-f5db12a03a91"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-aa16bb9b8200>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Mydrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Replace 'your_folder_name' with the actual folder name in your Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# where you want to save the files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/Mydrive')\n",
        "\n",
        "# Replace 'your_folder_name' with the actual folder name in your Google Drive\n",
        "# where you want to save the files.\n",
        "folder_name = 'H2V-Pipeline-portrait-video'\n",
        "output_folder = f'{folder_name}'\n",
        "\n",
        "import os\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Copy the files to the specified folder in Google Drive.\n",
        "# Replace 'output_vertical_video.mp4' and 'output_vertical_video_tracking.json'\n",
        "# with the correct file names, if necessary.\n",
        "try:\n",
        "    shutil.copy('output_vertical_video.mp4', output_folder)\n",
        "    print(f\"output_vertical_video.mp4 successfully copied to {output_folder}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"output_vertical_video.mp4 not found.\")\n",
        "\n",
        "try:\n",
        "    shutil.copy('output_vertical_video_tracking.json', output_folder)\n",
        "    print(f\"output_vertical_video_tracking.json successfully copied to {output_folder}\")\n",
        "except FileNotFoundError:\n",
        "    print(\"output_vertical_video_tracking.json not found.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8xzQgjduSsL6",
        "outputId": "94e36b48-11cf-4802-9159-6bfa2296e59a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}